{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd2f2b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os \n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, InputLayer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6022fb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"/Users/alya/Downloads/Dummy\"\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58c411c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 files belonging to 3 classes.\n",
      "Using 240 files for training.\n"
     ]
    }
   ],
   "source": [
    "img_size = 224\n",
    "batch_size =32\n",
    "train = tf.keras.utils.image_dataset_from_directory(\n",
    "  fpath,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_size, img_size),\n",
    "  batch_size=batch_size,\n",
    "  label_mode =\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b42e234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 224, 224, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 112, 112, 16)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 112, 112, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 56, 56, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 56, 56, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 28, 28, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 14, 14, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 175623    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 273063 (1.04 MB)\n",
      "Trainable params: 273063 (1.04 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Conv2D(16, kernel_size = (3,3), padding = 'same', activation = 'relu', input_shape = (224,224,3)))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2))) \n",
    "\n",
    "model.add(layers.Conv2D(32, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    " \n",
    "model.add(layers.Conv2D(64, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(128 ,kernel_size = (3,3) , padding = 'same' , activation='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2)))    \n",
    "    \n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(7, activation = 'softmax'))\n",
    "   \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2239f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    " # Adjust the rotation factor as needed\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),  # or \"vertical\" for vertical flipping\n",
    "    tf.keras.layers.RandomContrast(factor=0.2),# Adjust the contrast factor as needed\n",
    "    tf.keras.layers.RandomBrightness(factor=0.2) # Adjust the brightness factor as needed\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17f1095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b8981ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6d0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip and Rotation together\n",
    "\n",
    "# Define your data augmentation pipeline\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomRotation(factor=0.15), # Adjust the rotation factor as needed\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),  # or \"vertical\" for vertical flipping\n",
    "    tf.keras.layers.RandomContrast(factor=0.2)# Adjust the contrast factor as needed\n",
    "    tf.keras.layers.RandomBrightness(factor=0.2) # Adjust the brightness factor as needed\n",
    "])\n",
    "\n",
    "# Compile your model\n",
    "\n",
    "# Load your training data\n",
    "train_images = ...  # Load your training images\n",
    "\n",
    "# Apply data augmentation to the training images\n",
    "augmented_train_images = data_augmentation(train, training=True)\n",
    "\n",
    "# Fit/Train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "294b6aa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'random_flip_4' (type RandomFlip).\n\nAttempt to convert a value (<_BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>) with an unsupported type (<class 'tensorflow.python.data.ops.batch_op._BatchDataset'>) to a Tensor.\n\nCall arguments received by layer 'random_flip_4' (type RandomFlip):\n  • inputs=<_BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m augmented_train_images \u001b[38;5;241m=\u001b[39m \u001b[43mdata_augmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/DOG_IDENTIFIER/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/DOG_IDENTIFIER/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'random_flip_4' (type RandomFlip).\n\nAttempt to convert a value (<_BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>) with an unsupported type (<class 'tensorflow.python.data.ops.batch_op._BatchDataset'>) to a Tensor.\n\nCall arguments received by layer 'random_flip_4' (type RandomFlip):\n  • inputs=<_BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>\n  • training=True"
     ]
    }
   ],
   "source": [
    "augmented_train_images = data_augmentation(train, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853c2bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
